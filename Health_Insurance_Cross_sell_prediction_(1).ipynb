{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zylOP8IXa2m6"
      },
      "source": [
        "# **Project Title: Health-Insurance-Cross-Sell-Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7qSet8AeHvk"
      },
      "source": [
        "# **Problem Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Business Context**"
      ],
      "metadata": {
        "id": "_yFAkJTfCEXn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdMq3tjsej27"
      },
      "source": [
        "Our client is an insurance company that has provided health insurance to its customer now they need your help in building a model to predict wheather the policyholders(customers) from past year will also be interested in vehicle insurance provided by the company.An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation to specified loss,damage,illnes, or death in return for the payment of a specified premium.A premium is the sum of money that a customer needs to pay regularly to an insurance company for this guarantee.For example,you may pay a premium of Rs.5000 each year for a health insurance cover of Rs. 200,000/-so that if,God forbid,you fall ill and need to be hospitalised in that year,the insurance provider company will bear the cost of hospitalisation etc for upto Rs.200,000.Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/- that is where the concept of probabilities comes in picture.For example,like you,there may be 100 customers who would be paying a premium of Rs.5000 every year,but only a few of them (say 2-3) would get hospitalised that year and not everyone.This way everyone shares a risk of everyone else.\n",
        "\n",
        "Just like medical insurance,there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle,the insurance provider company will provide a compensation(called'sum assured')to the customer.\n",
        "\n",
        "Building a model to predict wheather a customer would be interested in Vehicle insurance is extremely helpful for the company because it can then accordingly plans its communication strategy to reach out to those customers and optimise its business model and revenue.\n",
        "\n",
        "Now,in order to predict,wheather the customer would be interested in vehicle insurance,you have information about demographics(gender,age,region code type),Vehicles(Vehicle Age,Damage),Policy(Premium,sourcing channel)etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6qqjp81nJZt"
      },
      "source": [
        "# **Dataset Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk2-6z4-nVMu"
      },
      "source": [
        "* id:- Unique ID for customer\n",
        "* Gender:- Male/Female\n",
        "* Age:- Age of customer\n",
        "* Driving License:- Customer has DL or not\n",
        "* Region_Code:- Unique code for the region of the customer\n",
        "* Previously_insured:-Customer already has vehicle insurance or not\n",
        "* Vehicle_age:-Age of the Vehicle\n",
        "* Vehicle_damage:-Past damage present or not\n",
        "* Annual_premium:-The amount customer need to pay as premium\n",
        "* PolicySalesChannel:-Anonymized code for the channel of outreaching to the customer ie.Different Agents,Overmail,Over Phone,In person,etc\n",
        "* Vintage:-Number of Days,Customer has been associated with the company\n",
        "* Response:- Customer is interested or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ7P9llHqwdm"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXI1JgBh6U-J"
      },
      "source": [
        "Insurance is an agreement by which an individual obtains protection against any losses from an insurance company against the risks of damage, financial losses, damage, illness, or death in return for the payment of a specified premium. In this project, we have an insurance details dataset which contains a total of 381109 rows and 12 features. We have a categorical dependent variable Response which represents whether a customer is interested in vehicle insurance or not. As an initial step, we checked for the null and duplicate values in our dataset. As there were no null and duplicate values present in our dataset, so data cleaning was not required. Further, we normalized the numerical columns for bringing them on the same scale.\n",
        "\n",
        "In Exploratory Data Analysis, we categorized the Age as YoungAge, MiddleAge, OldAge.Then we categorized Region_Code and Policy_Sales_Channel to extract some valuable information from these features. We explored the independent features using some plots.\n",
        "\n",
        "For Feature selection, we used Kendall's rank correlation coefficient for numerical features and for categorical features, we applied the Mutual Information technique.\n",
        "\n",
        "For Model prediction, we used supervised machine learning algorithms like Decision tree Classifier, AdaBoost, LightGBM, BaggingRegressor, NaiveBayes and Logistic regression. Then applied hyperparameter tuning techniques to obtain better accuracy and to avoid overfitting.\n",
        "\n",
        "So, without any further delay let’s move ahead!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2Kzd_FW6-fp"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCg_7zIH7Z7e"
      },
      "outputs": [],
      "source": [
        "#plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BrbC9wD7pC4"
      },
      "outputs": [],
      "source": [
        "#Machine learning models\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBLPPCni8O6A"
      },
      "outputs": [],
      "source": [
        "#Evaluation metrices\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import log_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1kruc-K8fj2"
      },
      "outputs": [],
      "source": [
        "#Tuning of hyperparameter\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingRandomSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2F2sw_V8qaX"
      },
      "outputs": [],
      "source": [
        "#Miscellaneous\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX2bvEfB9yjb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shTFE-63-SO6"
      },
      "outputs": [],
      "source": [
        "dataset=pd.read_csv('/content/drive/MyDrive/Almabetter/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Wcp5wY_Ywj"
      },
      "source": [
        "# **Health Insurance Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "src_Irii_jRF"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWfWJc6k_wQH"
      },
      "outputs": [],
      "source": [
        "dataset.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7Pps8hK_1fy"
      },
      "outputs": [],
      "source": [
        "dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iREzcDq7AENZ"
      },
      "outputs": [],
      "source": [
        "#checking of duplicate data\n",
        "dataset[dataset.duplicated()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FZbmHbMAWjU"
      },
      "source": [
        "# **Checking of Null values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ97TJxxAd-h"
      },
      "outputs": [],
      "source": [
        "dataset.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SYu1Zr5AySf"
      },
      "source": [
        "# **Observation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAzb4VttA65L"
      },
      "source": [
        "* As we can see, there are 12 columns and 381109 rows in our data set.\n",
        "\n",
        "* We do not have any null values in our dataset.\n",
        "\n",
        "* We have independent attributes that are 4 numerical and 5 category.\n",
        "\n",
        "* Response is a categorical column that is our dependent feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0gVpiwCBtz5"
      },
      "source": [
        "# **Data Cleaning and Refactoring**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4BRmSL7B1wk"
      },
      "source": [
        "Now we will be reformat and clean the data for smooth processing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl28XR6nCIIS"
      },
      "source": [
        "# **Finding outliers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PM80wb3CWV8"
      },
      "source": [
        "Let's examine the outliers in our dataset,if it's there or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO474LlVCtIh"
      },
      "outputs": [],
      "source": [
        "   def show_outliers(ds):\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(24,14))\n",
        "\n",
        "    sns.boxplot(ax = axes[0][0],y = 'Annual_Premium',x = 'Response', data = ds)\n",
        "    axes[0][0].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0][0].set_ylabel(ylabel = 'Annual_Premium', fontdict={'fontsize': 14})\n",
        "    axes[0][0].set_title('Annual_Premium', fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "    sns.boxplot(ax = axes[0][1],y = 'Age',x = 'Response', data = ds)\n",
        "    axes[0][1].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0][1].set_ylabel(ylabel = 'Age', fontdict={'fontsize': 14})\n",
        "    axes[0][1].set_title('Age', fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "    sns.boxplot(ax = axes[0][2],y = 'Vintage',x = 'Response', data = ds)\n",
        "    axes[0][2].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0][2].set_ylabel(ylabel = 'Vintage', fontdict={'fontsize': 14})\n",
        "    axes[0][2].set_title('Vintage', fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "\n",
        "    sns.distplot(ax = axes[1][0],x = ds['Annual_Premium'])\n",
        "    axes[1][0].set_xlabel(xlabel = 'Annual Premium', fontdict={'fontsize': 14})\n",
        "    axes[1][0].set_ylabel(ylabel = 'Density', fontdict={'fontsize': 14})\n",
        "    axes[1][0].set_title('Annual_Premium', fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "    sns.distplot(ax = axes[1][1],x = ds['Age'])\n",
        "    axes[1][1].set_xlabel(xlabel = 'Age', fontdict={'fontsize': 14})\n",
        "    axes[1][1].set_ylabel(ylabel = 'Density', fontdict={'fontsize': 14})\n",
        "    axes[1][1].set_title('Age', fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "    sns.distplot(ax = axes[1][2],x = ds['Vintage'])\n",
        "    axes[1][2].set_xlabel(xlabel = 'Vintage', fontdict={'fontsize': 14})\n",
        "    axes[1][2].set_ylabel(ylabel = 'Density', fontdict={'fontsize': 14})\n",
        "    axes[1][2].set_title('Vintage', fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "    plt.suptitle('Outliers', fontsize = 22, fontweight = 'bold' )\n",
        "\n",
        "show_outliers(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8eL_htaFLOi"
      },
      "source": [
        "* The above graphic suggests that the distribution of the annual premium is positively biased.\n",
        "\n",
        "* We can also see that Vintage has a rather approximately distribution from above.\n",
        "\n",
        "* There are some outliers in the age columns, but we won't deal with them because they won't have an impact on the outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLQ6I9vSFg9n"
      },
      "source": [
        "# **Treatment of outliers and Scalling of features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyYqDhEzFs3U"
      },
      "source": [
        "* We will use the quantile method to address outliers.\n",
        "* We will apply the MinMaxScaler technique for normalization to feature scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfVYFeBHGSRK"
      },
      "outputs": [],
      "source": [
        "def outlier_treatment(ds):\n",
        "    Q1=dataset['Annual_Premium'].quantile(0.25)\n",
        "    Q3=dataset['Annual_Premium'].quantile(0.75)\n",
        "    IQR=Q3-Q1\n",
        "\n",
        "    Lower_Whisker = Q1-1.5*IQR\n",
        "    Upper_Whisker = Q3+1.5*IQR\n",
        "    dataset['Annual_Premium_Treated'] = np.where(dataset['Annual_Premium']>Upper_Whisker, Upper_Whisker, dataset['Annual_Premium'])\n",
        "\n",
        "def scale_features(ds):\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    dataset['Annual_Premium_Treated'] = scaler.fit_transform(dataset['Annual_Premium_Treated'].values.reshape(-1,1))\n",
        "    dataset['Vintage_Treated'] = scaler.fit_transform(dataset['Vintage'].values.reshape(-1,1))\n",
        "\n",
        "outlier_treatment(dataset)\n",
        "scale_features(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-Ey-SE0G1n5"
      },
      "outputs": [],
      "source": [
        "def show_ann_prem_outliers(ds):\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(17,9))\n",
        "    sns.boxplot(ax = axes[0], y = 'Annual_Premium_Treated',x = 'Response', data = ds)\n",
        "    axes[0].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[0].set_title('Annual Premium Treated', fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "    sns.distplot(ax = axes[1], x = ds['Annual_Premium_Treated'], color='brown')\n",
        "    axes[1].set_xlabel(xlabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[1].set_ylabel(ylabel = 'Density', fontdict={'fontsize': 14})\n",
        "    axes[1].set_title('Annual Premium Treated', fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "\n",
        "show_ann_prem_outliers(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul5CsKV8ICHx"
      },
      "source": [
        "* We can see from the above plots that the Annual Premium has no more outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU0_c7-TIX2d"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuJJIUy3EsEY"
      },
      "outputs": [],
      "source": [
        "def show_distribution_numerical_features(ds):\n",
        "    fig, axes = plt.subplots(2,2, figsize=(22,18))\n",
        "\n",
        "    sns.countplot(ax = axes[0][0],x = 'Age', data = ds, hue='Response')\n",
        "    axes[0][0].set_xlabel(xlabel = 'Age', fontdict={'fontsize': 12})\n",
        "    axes[0][0].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 12})\n",
        "    axes[0][0].set_title('Age', fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "    sns.countplot(ax = axes[0][1],x = 'Region_Code', data = ds, hue='Response')\n",
        "    axes[0][1].set_xlabel(xlabel = 'Region_Code', fontdict={'fontsize': 12})\n",
        "    axes[0][1].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 12})\n",
        "    axes[0][1].set_title('Region_Code',fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "    sns.countplot(ax = axes[1][0],x = 'Policy_Sales_Channel', data = ds, hue='Response')\n",
        "    axes[1][0].set_xlabel(xlabel = 'Policy_Sales_Channel', fontdict={'fontsize': 12})\n",
        "    axes[1][0].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 12})\n",
        "    axes[1][0].set_title('Policy_Sales_Channel',fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "    sns.histplot(ax = axes[1][1], x = dataset['Vintage'],data = ds, hue='Response')\n",
        "    axes[1][1].set_xlabel(xlabel = 'Vintage', fontdict={'fontsize': 12})\n",
        "    axes[1][1].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 12})\n",
        "    axes[1][1].set_title('Vintage',fontdict={'fontsize': 15,  'fontweight' :'bold'})\n",
        "\n",
        "    plt.suptitle('Distribution of Numerical Features', fontsize = 24, fontweight = 'bold' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kikhTxkQF4Qx"
      },
      "outputs": [],
      "source": [
        "def show_violin_distribution(ds):\n",
        "\n",
        "    sns.catplot(y = 'Age', data = ds, x='Response', kind = 'violin')\n",
        "    plt.xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    plt.ylabel(ylabel = 'Age', fontdict={'fontsize': 14})\n",
        "    plt.title('Age Distribution', fontdict={'fontsize': 20, 'fontweight':'bold'})\n",
        "\n",
        "    sns.catplot(y = 'Region_Code', data = ds, x='Response', kind = 'violin')\n",
        "    plt.xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    plt.ylabel(ylabel = 'Region_Code', fontdict={'fontsize': 14})\n",
        "    plt.title('Region Code Distribution', fontdict={'fontsize': 20, 'fontweight':'bold'})\n",
        "\n",
        "    sns.catplot(y = 'Policy_Sales_Channel', data = ds, x='Response', kind = 'violin')\n",
        "    plt.xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    plt.ylabel(ylabel = 'Policy_Sales_Channel', fontdict={'fontsize': 14})\n",
        "    plt.title('Policy Sales Channel Distribution', fontdict={'fontsize': 20, 'fontweight':'bold'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXfOlR3fINu1"
      },
      "outputs": [],
      "source": [
        "def convert_numerical_to_categorical(ds):\n",
        "    # Categorizing Age feature\n",
        "    ds['Age_Group'] = ds['Age'].apply(lambda x:'YoungAge' if x >= 20 and x<=45 else 'MiddleAge' if x>45 and x<=65 else 'OldAge')\n",
        "\n",
        "    # Categorizing Policy_Sales_Channel feature\n",
        "    x = ds['Policy_Sales_Channel'].value_counts().apply(lambda x: 'Channel_A' if x>100000 else 'Channel_B' if 74000<x<100000 else 'Channel_C' if 10000<x<=74000 else 'Channel_D')\n",
        "    res = dict(zip(x.keys(),x.values))\n",
        "    ds['Policy_Sales_Channel_Categorical'] = ds['Policy_Sales_Channel'].map(res)\n",
        "\n",
        "    # Categorizing Region Code feature\n",
        "    x = ds['Region_Code'].value_counts().apply(lambda x: 'Region_A' if x>100000 else 'Region_B' if x>11000 and x<340000 else 'Region_C')\n",
        "    res = dict(zip(x.keys(),x.values))\n",
        "    ds['Region_Code_Categorical'] = ds['Region_Code'].map(res)\n",
        "    # df.Region_Code_Categorical.value_counts()\n",
        "\n",
        "convert_numerical_to_categorical(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx9xZMw5MWzR"
      },
      "outputs": [],
      "source": [
        "def show_distribution_num_to_cat(ds):\n",
        "    fig, axes = plt.subplots(1,3, figsize=(22,8))\n",
        "\n",
        "    sns.countplot(ax = axes[0],x = 'Age_Group', data = ds, hue='Response')\n",
        "    axes[0].set_xlabel(xlabel = 'Age_Group', fontdict={'fontsize': 14})\n",
        "    axes[0].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[0].set_title('Age', fontdict={'fontsize': 15})\n",
        "\n",
        "    sns.countplot(ax = axes[1],x = 'Region_Code_Categorical', data = ds, hue='Response')\n",
        "    axes[1].set_xlabel(xlabel = 'Region_Code_Categorical', fontdict={'fontsize': 14})\n",
        "    axes[1].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[1].set_title('Region_Code',fontdict={'fontsize': 15})\n",
        "\n",
        "    sns.countplot(ax = axes[2],x = 'Policy_Sales_Channel_Categorical', data = ds, hue='Response')\n",
        "    axes[2].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "    axes[2].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[2].set_title('Policy_Sales_Channel',fontdict={'fontsize': 15})\n",
        "\n",
        "    plt.suptitle('Distribution of Categorical Features', fontsize = 22, fontweight = 'bold' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNOHwKsfNYHT"
      },
      "outputs": [],
      "source": [
        "def show_gender_response_relation(ds):\n",
        "    sns.catplot(x=\"Response\", hue=\"Gender\", kind=\"count\",\n",
        "                palette=\"pastel\",\n",
        "                data=ds)\n",
        "    plt.xlabel('Response', fontdict={'fontsize':15})\n",
        "    plt.ylabel('Count',fontdict={'fontsize': 16})\n",
        "    plt.title('Response V/S Gender', fontdict={'fontsize': 18, 'fontweight':'bold'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7XDbj1-NuXO"
      },
      "outputs": [],
      "source": [
        "def show_age_relations(ds):\n",
        "    fig, axes = plt.subplots(1,3, figsize=(27,9))\n",
        "    sns.countplot(ax = axes[0],x=\"Response\", hue=\"Age_Group\", palette=\"pastel\",\n",
        "            data=ds)\n",
        "    axes[0].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[0].set_title('Age_Group', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.histplot(ax = axes[1],binwidth=0.5, x=\"Age_Group\",\n",
        "                 hue=\"Previously_Insured\", data=ds,\n",
        "                 stat=\"count\", multiple=\"stack\")\n",
        "    axes[1].set_xlabel(xlabel = 'Age_Group', fontdict={'fontsize': 14})\n",
        "    axes[1].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[1].set_title('Age_Group V/S Previously_Insured', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.lineplot(ax = axes[2], x=\"Age\",y=\"Annual_Premium_Treated\",\n",
        "                 data=ds,hue=\"Gender\")\n",
        "    axes[2].set_xlabel(xlabel = 'Age', fontdict={'fontsize': 14})\n",
        "    axes[2].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[2].set_title('Age V/S Annual Premium Treated', fontdict={'fontsize': 15, 'fontweight':'bold'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1ZDvSx6Ocz3"
      },
      "outputs": [],
      "source": [
        "def vehicle_damage_distribution(ds):\n",
        "    fig = px.pie(ds, values='Response', names='Vehicle_Damage', title='Vehicle Damage Distribution')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjdtPz72OusL"
      },
      "outputs": [],
      "source": [
        "def show_vehicle_damage_relations(ds):\n",
        "    fig, axes = plt.subplots(1,2, figsize=(24,9))\n",
        "    sns.pointplot(ax = axes[0], x=\"Vehicle_Damage\", y=\"Response\", hue=\"Vehicle_Age\",\n",
        "             data=ds)\n",
        "    axes[0].set_xlabel(xlabel = 'Vehicle_Damage', fontdict={'fontsize': 14})\n",
        "    axes[0].set_ylabel(ylabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0].set_title('Vehicle_Damage V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.pointplot(x = 'Vehicle_Damage', y = 'Annual_Premium_Treated', data=ds)\n",
        "    axes[1].set_xlabel(xlabel = 'Vehicle_Damage', fontdict={'fontsize': 14})\n",
        "    axes[1].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[1].set_title('Vehicle_Damage V/S Annual_Premium_Treated', fontdict={'fontsize': 15, 'fontweight':'bold'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkND0OHPqVg"
      },
      "outputs": [],
      "source": [
        "def vehicle_age_distribution(ds):\n",
        "    plt.figure(figsize=(12, 9))\n",
        "    sns.countplot(x = 'Vehicle_Age', hue='Response', data = ds, palette=\"Dark2\")\n",
        "    plt.xlabel(xlabel = 'Vehicle_Age', fontdict={'fontsize': 14})\n",
        "    plt.ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    plt.title('Vehicle_Age', fontdict={'fontsize': 15, 'fontweight':'bold'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dn0R632RLaS"
      },
      "outputs": [],
      "source": [
        "def show_vehicle_age_relation(ds):\n",
        "    fig, axes = plt.subplots(2,3, figsize=(22,15))\n",
        "\n",
        "    sns.barplot(ax = axes[0][0], x = 'Vehicle_Age', y='Response', data = ds)\n",
        "    axes[0][0].set_xlabel(xlabel = 'Vehicle_Age', fontdict={'fontsize': 14})\n",
        "    axes[0][0].set_ylabel(ylabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0][0].set_title('Vehicle_Age V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.pointplot(ax = axes[0][1], y = 'Response', x = 'Vehicle_Age', hue = 'Vehicle_Damage', data=ds)\n",
        "    axes[0][1].set_xlabel(xlabel = 'Vehicle_Age', fontdict={'fontsize': 14})\n",
        "    axes[0][1].set_ylabel(ylabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0][1].set_title('Vehicle_Age V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.pointplot(ax = axes[0][2], y = 'Response', x = 'Vehicle_Age', hue = 'Region_Code_Categorical', data=ds)\n",
        "    axes[0][2].set_xlabel(xlabel = 'Vehicle_Age', fontdict={'fontsize': 14})\n",
        "    axes[0][2].set_ylabel(ylabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0][2].set_title('Vehicle_Age V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.pointplot(ax = axes[1][0], y = 'Response', x = 'Vehicle_Age', hue = 'Policy_Sales_Channel_Categorical', data=ds )\n",
        "    axes[1][0].set_xlabel(xlabel = 'Vehicle_Age', fontdict={'fontsize': 14})\n",
        "    axes[1][0].set_ylabel(ylabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[1][0].set_title('Vehicle_Age V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "    sns.boxplot(ax = axes[1][1], y = 'Annual_Premium_Treated', x = 'Vehicle_Age', hue = 'Vehicle_Damage', data=ds)\n",
        "    axes[1][1].set_xlabel(xlabel = 'Vehicle_Age', fontdict={'fontsize': 14})\n",
        "    axes[1][1].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[1][1].set_title('Vehicle_Age V/S Annual_Premium_Treated', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.stripplot(ax = axes[1][2], y = 'Annual_Premium_Treated', x = 'Vehicle_Age', hue = 'Vehicle_Damage', data=ds)\n",
        "    axes[1][2].set_xlabel(xlabel = 'Vehicle_Age', fontdict={'fontsize': 14})\n",
        "    axes[1][2].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[1][2].set_title('Vehicle_Age V/S Annual_Premium_Treated', fontdict={'fontsize': 15, 'fontweight':'bold'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc1gZ9QDRqG_"
      },
      "outputs": [],
      "source": [
        "def show_annual_premium_relation(ds):\n",
        "    fig, axes = plt.subplots(2,2, figsize=(18,14))\n",
        "\n",
        "    sns.pointplot(ax = axes[0][0], x = 'Response', y = 'Annual_Premium_Treated', data = ds)\n",
        "    axes[0][0].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0][0].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[0][0].set_title('Annual_Premium_Treated V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.violinplot(ax = axes[0][1], x = 'Response', y = 'Annual_Premium_Treated', data = ds)\n",
        "    axes[0][1].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[0][1].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[0][1].set_title('Annual_Premium_Treated V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.swarmplot(ax = axes[1][0], x = 'Response', y = 'Annual_Premium_Treated', data = ds[:1000])\n",
        "    axes[1][0].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[1][0].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[1][0].set_title('Annual_Premium_Treated V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.stripplot(ax = axes[1][1], x = 'Response', y = 'Annual_Premium_Treated', data = ds)\n",
        "    axes[1][1].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[1][1].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[1][1].set_title('Annual_Premium_Treated V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOaTxbbTSqmg"
      },
      "outputs": [],
      "source": [
        "def show_annual_premium_with_age_group(ds):\n",
        "    fig, axes = plt.subplots(1,2, figsize=(15,8))\n",
        "\n",
        "    sns.barplot(ax = axes[0],y = 'Annual_Premium_Treated', x = 'Age_Group', data= dataset)\n",
        "    axes[0].set_xlabel(xlabel = 'Age_Group', fontdict={'fontsize': 14})\n",
        "    axes[0].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[0].set_title('Annual_Premium_Treated V/S Age_Group', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.violinplot(ax = axes[1], y = 'Annual_Premium_Treated', x = 'Age_Group', data= ds)\n",
        "    axes[1].set_xlabel(xlabel = 'Age_Group', fontdict={'fontsize': 14})\n",
        "    axes[1].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[1].set_title('Annual_Premium_Treated V/S Age_Group', fontdict={'fontsize': 15, 'fontweight':'bold'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C73JvHC0TgqR"
      },
      "outputs": [],
      "source": [
        "def show_age_annual_premium_relation(ds):\n",
        "\n",
        "    plt.figure(figsize = (16,9))\n",
        "    plt.hexbin(data=dataset, x='Age',y='Annual_Premium_Treated',gridsize = 30, cmap ='Greens')\n",
        "    plt.title('Annual Premium V/S Age', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "    plt.ylabel('Annual Premium Treated',fontdict={'fontsize': 14})\n",
        "    plt.xlabel('Age', fontdict={'fontsize': 14})\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    fig = px.scatter(ds, x=\"Age\", y=\"Annual_Premium\",\n",
        "                    color=\"Region_Code_Categorical\",\n",
        "                    size_max=180,opacity=0.3, title='Age V/S Annual Premium')\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgHAYEhiUMBZ"
      },
      "outputs": [],
      "source": [
        "def age_group_distribution(ds):\n",
        "    fig, axes = plt.subplots(1,3, figsize=(17,9))\n",
        "\n",
        "    colors = sns.color_palette('pastel')[0:4]\n",
        "    explode = (0.01, 0.25, 0.01)\n",
        "    axes[0].pie( x= ds.groupby('Age_Group')['Response'].sum(),explode=explode,\n",
        "            labels=ds['Age_Group'].unique(), colors=colors, autopct='%1.1f%%',\n",
        "            shadow=True);\n",
        "    axes[0].set_title('with Response', fontsize = 15, fontweight ='bold', pad=15)\n",
        "\n",
        "    axes[1].pie(x=ds.groupby('Age_Group')['Annual_Premium'].sum(),explode=explode,\n",
        "            labels=ds['Age_Group'].unique(), colors=colors, autopct='%1.1f%%',\n",
        "            shadow=True);\n",
        "    axes[1].set_title('with Annual_Premium', fontsize = 15, fontweight ='bold', pad=15)\n",
        "\n",
        "    axes[2].pie(x=ds.groupby('Age_Group')['Previously_Insured'].sum(),explode=explode,\n",
        "            labels=ds['Age_Group'].unique(), colors=colors, autopct='%1.1f%%',\n",
        "            shadow=True);\n",
        "    axes[2].set_title('with Previously_Insured', fontsize = 15, fontweight ='bold', pad=15)\n",
        "\n",
        "    plt.suptitle('Age Group Distribution',fontsize = 20, fontweight ='bold')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww_3LiBDU2so"
      },
      "outputs": [],
      "source": [
        "def show_region_code_distribution(ds):\n",
        "\n",
        "    colors = sns.color_palette('pastel')[0:4]\n",
        "    explode = (0.01, 0.01, 0.01)\n",
        "\n",
        "    fig, axes = plt.subplots(1,2, figsize=(15,6))\n",
        "    axes[0].pie(x=ds.groupby('Region_Code_Categorical')['Vintage'].sum(),explode=explode,\n",
        "                labels=dataset['Region_Code_Categorical'].unique(), colors=colors,autopct='%1.1f%%',\n",
        "                shadow=True);\n",
        "    axes[0].set_title('with Vintage', fontsize = 15, fontweight ='bold', pad=15)\n",
        "\n",
        "    axes[1].pie(x=ds.groupby('Region_Code_Categorical')['Annual_Premium_Treated'].sum(),explode=explode,\n",
        "                labels=dataset['Region_Code_Categorical'].unique(), colors=colors, autopct='%1.1f%%',\n",
        "                shadow=True);\n",
        "    axes[1].set_title('with Annual_Premium', fontsize = 15, fontweight ='bold', pad=15)\n",
        "\n",
        "    plt.suptitle('Region Code Distribution',fontsize = 15, fontweight ='bold')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6iJOPTiVrFD"
      },
      "outputs": [],
      "source": [
        "def show_policy_sales_channel_relation(ds):\n",
        "\n",
        "    fig, axes = plt.subplots(2,3, figsize=(22,15))\n",
        "\n",
        "    sns.pointplot(ax = axes[0][0], x='Policy_Sales_Channel_Categorical', y='Vintage',data=ds)\n",
        "    axes[0][0].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "    axes[0][0].set_ylabel(ylabel = 'Vintage', fontdict={'fontsize': 14})\n",
        "    axes[0][0].set_title('Policy_Sales_Channel V/S Vintage',\n",
        "                         fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    sns.pointplot(ax = axes[0][1], x='Policy_Sales_Channel_Categorical', y='Annual_Premium_Treated',data=ds)\n",
        "    axes[0][1].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "    axes[0][1].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})\n",
        "    axes[0][1].set_title('Policy_Sales_Channel V/S Annual_Premium_Treated',\n",
        "                         fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    ds['Policy_Sales_Channel_Categorical'].value_counts().plot(ax = axes[0][2] ,kind='barh')\n",
        "    axes[0][2].set_xlabel(xlabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[0][2].set_ylabel(ylabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "    axes[0][2].set_title('Policy_Sales_Channel', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "\n",
        "    sns.histplot(ax = axes[1][0],x=\"Policy_Sales_Channel_Categorical\", hue=\"Response\", data=ds, stat=\"count\",\n",
        "                 multiple=\"stack\",binwidth=0.5)\n",
        "    axes[1][0].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "    axes[1][0].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "    axes[1][0].set_title('Policy_Sales_Channel', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "    groupPolicySalesBySum=ds.groupby(by=[\"Policy_Sales_Channel_Categorical\"]).sum().reset_index()\n",
        "    sns.barplot(ax = axes[1][1], x=\"Policy_Sales_Channel_Categorical\", y=\"Response\", data=groupPolicySalesBySum)\n",
        "    axes[1][1].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "    axes[1][1].set_ylabel(ylabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[1][1].set_title('Policy_Sales_Channel V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "\n",
        "    sns.barplot(ax = axes[1][2], x='Policy_Sales_Channel_Categorical', y='Response', data=ds, hue='Region_Code_Categorical')\n",
        "    axes[1][2].set_xlabel(xlabel = 'Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})\n",
        "    axes[1][2].set_ylabel(ylabel = 'Response', fontdict={'fontsize': 14})\n",
        "    axes[1][2].set_title('Policy_Sales_Channel V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chji7x68Xd22"
      },
      "outputs": [],
      "source": [
        "def count_each_categorical_feature(ds):\n",
        "    categorical_columns = ['Gender', 'Age_Group', 'Region_Code_Categorical', 'Previously_Insured', 'Vehicle_Age','Vehicle_Damage', 'Policy_Sales_Channel_Categorical']\n",
        "\n",
        "    fig, axes =  plt.subplots(2, 7, figsize=(45, 15))\n",
        "    for i in range(7):\n",
        "        sns.countplot(data = ds[ds['Response']==1], x=categorical_columns[i], ax=axes[0][i])\n",
        "        axes[0][i].set_xlabel(xlabel = categorical_columns[i], fontdict={'fontsize': 14})\n",
        "        axes[0][i].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "        axes[0][i].set_title(categorical_columns[i],\n",
        "                         fontdict={'fontsize': 15, 'fontweight':'bold'})\n",
        "\n",
        "        sns.countplot(data = ds[ds['Response']==0], x=categorical_columns[i], ax=axes[1][i])\n",
        "\n",
        "        axes[1][i].set_xlabel(xlabel = categorical_columns[i], fontdict={'fontsize': 14})\n",
        "        axes[1][i].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})\n",
        "        axes[1][i].set_title(categorical_columns[i],\n",
        "                         fontdict={'fontsize': 15, 'fontweight':'bold'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOGJXtanXxqR"
      },
      "source": [
        "# **Exploring the numerical feature**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbK9UnNlYWmb"
      },
      "source": [
        "We have four numerical features: Vintage, Region_Code, Policy_Sales_Channel, and Age. Let's explore these features without further ado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzVjLJtjYsSR"
      },
      "outputs": [],
      "source": [
        "show_distribution_numerical_features(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjUdZ1akY2Gw"
      },
      "outputs": [],
      "source": [
        "show_violin_distribution(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR-3iNeOd9-Y"
      },
      "source": [
        "# **From the above graphical representation, we can draw the following conclusions:**\n",
        "\n",
        "* As we can see, there is a great deal of data dispersion in the Age feature, so we can divide it into categories like YoungAge, MiddleAge, and OldAge to acquire better insights.\n",
        "\n",
        "* The categories for Region Code and Policy_Sales_Channel are same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbV9RdIKeM3M"
      },
      "source": [
        "# **Converting Numerical columns to Categorical**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1FdF8J3eajN"
      },
      "outputs": [],
      "source": [
        "show_distribution_num_to_cat(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWlNfUU2etZ7"
      },
      "source": [
        "### **Observations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKRCKj7UfKDH"
      },
      "source": [
        "* We can tell that YoungAge group customers are less likely to be interested in purchasing vehicle insurance.\n",
        "\n",
        "* Similar to this, customers from Region_C and Channel_A had the highest likelihood of forgoing vehicle insurance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjLfGI7gfU9L"
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NklyPhhgfb2f"
      },
      "source": [
        "### **Gender Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e8_AHVffjXC"
      },
      "outputs": [],
      "source": [
        "show_gender_response_relation(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLZmSeN9gI8D"
      },
      "source": [
        "* According to the above plot we can say that, there are more male customers in our data set than female customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR4Juo0LgPkh"
      },
      "source": [
        "### **Exploring the Age Feature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REPeGL2ygba5"
      },
      "outputs": [],
      "source": [
        "show_age_relations(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPHLjHQ6gsR7"
      },
      "source": [
        "### **Observation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L99Ia6sfhSCi"
      },
      "source": [
        "* We can see the Responses from the different Age_Groups from the first figure.\n",
        "\n",
        "* The second graph displays the percentage of customers in each age group who have or do not have auto insurance.\n",
        "\n",
        "* We can state that consumers in the YoungAge and OldAge age groups are equally likely to have or not have auto insurance, however customers in the MiddleAge group have the highest odds of not having had auto insurance in the past.\n",
        "\n",
        "* The third plot shows how the annual_premium for both male and female clients is correlated with their age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B3l96n0iBfq"
      },
      "source": [
        "# **Exploring Vehicle Damage**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ype-j0oMiW1e"
      },
      "outputs": [],
      "source": [
        "vehicle_damage_distribution(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLqVNsVjk91G"
      },
      "outputs": [],
      "source": [
        "!pip uninstall scikit-learn -y\n",
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzUqy2CGiv1i"
      },
      "outputs": [],
      "source": [
        "show_vehicle_damage_relations(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuTJPtBznP3E"
      },
      "source": [
        "# **Observations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-yxBPVeoFi5"
      },
      "source": [
        "* The percentage of clients who took the insurance and had damaged or undamaged vehicles is depicted in a pie chart.\n",
        "\n",
        "* From the first point plot, regardless of the vehicle age group, there is a higher likelihood that you will purchase auto insurance if your vehicle is damaged. The likelihood of purchasing auto insurance rises as vehicle age increases.\n",
        "\n",
        "* According to the second point plot, consumers with damaged vehicles pay a comparably higher Annual_Premium."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtc7wtDvoiZC"
      },
      "source": [
        "## **Exploring Vehicle Age Feature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y33HVld6o4lP"
      },
      "outputs": [],
      "source": [
        "vehicle_age_distribution(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH5FZ_P9pBuT"
      },
      "outputs": [],
      "source": [
        "show_vehicle_age_relation(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wulFov7-pZiU"
      },
      "source": [
        "### **Observation:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTfSl26PqTB3"
      },
      "source": [
        "* The first bar plot shows how many members of the VehicleAge group purchased or declined vehicle insurance.\n",
        "\n",
        "* The possibility of purchasing vehicle insurance for a specific VehicleAge group is shown in the first two plots of the grid above.\n",
        "\n",
        "* In the third plot of the grid above, it is possible to purchase vehicle insurance for a certain VehicleAge group of vehicles based on their RegionCode.\n",
        "\n",
        "* The fourth plot in the grid above illustrates the option of purchasing vehicle insurance for a specific VehicleAge group based on their PolicySalesChannel group.\n",
        "\n",
        "* From the box plot of the above grid, we can notice the relation of Vehicle_Age group and Annual_Premium based on their Vehicle_Damage response.\n",
        "\n",
        "* The strip plot demonstrates that the customers having vehicle age >2 Years have the higher chances of taking vehicle insurance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7K2zvtXru9n"
      },
      "source": [
        "## **Exploring Annual Premium**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5cGxSQOr4HN"
      },
      "outputs": [],
      "source": [
        "show_annual_premium_relation(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkp4Aa5dsDaF"
      },
      "source": [
        "### **Observations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "josUegbqtvCP"
      },
      "source": [
        "* We can say from the point plot that people are more likely to purchase vehicle insurance if the Annual_Premium is higher.\n",
        "\n",
        "* The violin plot's second plot shows the same thing as well.\n",
        "\n",
        "* Third plot shows the plattern of responses based on the Annual_Premium.\n",
        "\n",
        "* Fourth plot is the strip plot for Annual_Premium and Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Bw7TzduDBc"
      },
      "source": [
        "## **Annual Premium and Age**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akDImYsVuLHc"
      },
      "outputs": [],
      "source": [
        "show_annual_premium_with_age_group(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EQfNrB_uhW3"
      },
      "source": [
        "* The distribution of the annual premium on the basis of age group is depicted in the two plots above, the bar and the violin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBGQqFKYu9a-"
      },
      "outputs": [],
      "source": [
        "show_age_annual_premium_relation(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJDXfnZjx15m"
      },
      "source": [
        "# **Observations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs9gj-dVyJ1e"
      },
      "source": [
        "* The first plot shows each person's Annual_Premium according to age.\n",
        "\n",
        "* The identical data points are displayed in the second plot, but they are categorized by Region_Code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmxOLVBTytd3"
      },
      "source": [
        "## **Age Group**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZQe0IxUy-pF"
      },
      "outputs": [],
      "source": [
        "age_group_distribution(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCe69QPOzIKC"
      },
      "outputs": [],
      "source": [
        "show_region_code_distribution(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJCI8K-lzPUX"
      },
      "source": [
        "### **Observations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQWXGRfD0v1k"
      },
      "source": [
        "* The distribution of Age_Group in the Data set based on Response, Annual_Premium, and Previously_Insured is shown in the above three pie graphs up top.\n",
        "\n",
        "* Based on Vintage and Annual_Premium, the two pie plots up above shows how Region_Code is distributed throughout the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUEMsTS51PcZ"
      },
      "source": [
        "## **Exploring Policy Sales Channel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JQ5Vuk91pCA"
      },
      "outputs": [],
      "source": [
        "show_policy_sales_channel_relation(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRB6Jhz81_ku"
      },
      "source": [
        "### **Observations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLuSYQ9F2PVS"
      },
      "source": [
        "* The distribution of Policy_Sales_Channel based on Vintage and Annual_Premium_Treated is shown in the two point plot graphs.\n",
        "\n",
        "* Based on responses,the next three bar plots shows the number of data points belonging to a particular channel.\n",
        "\n",
        "* The last bar plot displays, based on Policy_Sales_Channel and Region_Code, the likelihood that a consumer would purchase auto insurance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WziyKJVw3u1D"
      },
      "source": [
        "## **Distribution plots based on features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjH63cLs4Cht"
      },
      "source": [
        "* The distribution of data points depending on different features is shown in the charts below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbLO7sKd4dez"
      },
      "outputs": [],
      "source": [
        "count_each_categorical_feature(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDO6E8dn4nOQ"
      },
      "source": [
        "## **Dropping Extra columns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egGwPRvp45wq"
      },
      "source": [
        "* Age, Region Code, Annual Premium, Policy Sales Channel, and Vintage features can now be drop because we have previously classified them in our data set.\n",
        "\n",
        "* 'ID' and 'Driving_License' can also be removed because they don't offer any valuable information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSHVWXGx6A00"
      },
      "outputs": [],
      "source": [
        "dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKtYU49r6Fce"
      },
      "outputs": [],
      "source": [
        "# Dropping Unnecessary Columns\n",
        "cols_to_drop = ['id', 'Age', 'Driving_License',\n",
        "                'Region_Code', 'Annual_Premium',\n",
        "       'Policy_Sales_Channel', 'Vintage']\n",
        "dataset.drop(columns = cols_to_drop,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUkDeiA8i11y"
      },
      "source": [
        "# **Feature Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEdM6l3ii_mk"
      },
      "source": [
        "# **Numeric Feature Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_LqKT0QjXJf"
      },
      "source": [
        "Now will see the Kendall's correlation between numerical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF2ayUyOjuwS"
      },
      "outputs": [],
      "source": [
        "def numeric_feature_selection(ds):\n",
        "    plt.rcParams['figure.figsize'] = 14.6,8.29\n",
        "    numeric_features = ['Annual_Premium_Treated','Vintage_Treated']\n",
        "\n",
        "    sns.heatmap(ds[numeric_features].corr(method = 'kendall'),\n",
        "                cmap=\"YlGnBu\",annot=True)\n",
        "    plt.title('Correlation Between Numeric Features', fontdict={'fontsize':22,'fontweight':'bold'})\n",
        "\n",
        "numeric_feature_selection(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkVUFaoFlKJ7"
      },
      "source": [
        "In above figure we got two numeric features-Annual_Premium_Treated and Vintage_Treated\n",
        "\n",
        "* Since there is no connection between these two features,we will proceed with both of them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoOObKNHoGzI"
      },
      "source": [
        "## **Categorical Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnS3lq5BoRKN"
      },
      "source": [
        "Now we will see the categorical importance of these two features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uurDadeNom_Y"
      },
      "outputs": [],
      "source": [
        "categorical_features = ['Gender','Age_Group','Region_Code_Categorical','Previously_Insured',\n",
        "                        'Vehicle_Age','Vehicle_Damage','Policy_Sales_Channel_Categorical']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVZ1JueKo3K7"
      },
      "outputs": [],
      "source": [
        "def make_features_numeric(ds):\n",
        "    global numeric_ds\n",
        "    numeric_ds = ds.copy()\n",
        "    numeric_ds['Gender'] = numeric_ds['Gender'].apply(lambda x: 1 if x == 'Male' else 0)\n",
        "    numeric_ds['Age_Group'] = numeric_ds['Age_Group'].apply(lambda x: 1 if x == 'YoungAge' else 2 if x == 'MiddleAge' else 3)\n",
        "    numeric_ds['Vehicle_Age'] = numeric_ds['Vehicle_Age'].apply(lambda x: 1 if x == 'New' else 2 if x == 'Latest' else 3)\n",
        "    numeric_ds['Vehicle_Damage'] = numeric_ds['Vehicle_Damage'].apply(lambda x: 0 if x == 'Y' else 1)\n",
        "    numeric_ds['Policy_Sales_Channel_Categorical'] = numeric_ds['Policy_Sales_Channel_Categorical'].apply(lambda x: 1 if x == 'A' else 2 if x == 'B' else 3 if x=='C' else 4)\n",
        "    numeric_ds['Region_Code_Categorical'] = numeric_ds['Region_Code_Categorical'].apply(lambda x: 1 if x == 'A' else 2 if x == 'B' else 3)\n",
        "\n",
        "make_features_numeric(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fypQWL2gpMNm"
      },
      "source": [
        "## **Mutual Information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7qrY2UgpQwX"
      },
      "source": [
        "Mutual information measures the amount of information we get from one variable by observing the values of the second variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ6VVSeRqQf6"
      },
      "outputs": [],
      "source": [
        "def mutual_info(ds):\n",
        "    X = ds.copy()\n",
        "    y = X.pop(\"Response\")\n",
        "    X.drop(columns = ['Annual_Premium_Treated','Vintage_Treated'], inplace = True)\n",
        "\n",
        "    x_train, x_test, y_train, y_test=train_test_split(X,y,test_size=0.3)\n",
        "\n",
        "    high_score_features = []\n",
        "    feature_scores = mutual_info_classif( x_train, y_train,  random_state=0)\n",
        "\n",
        "    column_score = {}\n",
        "    columns = []\n",
        "    scores = []\n",
        "    for score, f_name in sorted(zip(feature_scores, x_train.columns), reverse=True):\n",
        "        columns.append(f_name)\n",
        "        scores.append(score)\n",
        "        high_score_features.append(f_name)\n",
        "\n",
        "    column_score['Feature'] = columns\n",
        "    column_score['Score'] = scores\n",
        "\n",
        "    return pd.DataFrame(data = column_score)\n",
        "\n",
        "def show_feature_importance_through_mi(ds):\n",
        "    sns.barplot(data = mutual_info(ds), x = 'Feature', y='Score')\n",
        "    plt.title('Feature Importance Using Mutual Information',\n",
        " fontdict={'fontsize':22,'fontweight':'bold'})\n",
        "    plt.xlabel('Features', fontdict={'fontsize':18})\n",
        "    plt.ylabel('Score', fontdict={'fontsize':18})\n",
        "    plt.xticks(rotation=90)\n",
        "\n",
        "show_feature_importance_through_mi(numeric_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VGn-Z-GsLJ1"
      },
      "source": [
        "* According to the above bar plot, Previously_Insured is the most important feature and has the most influence on the dependent feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXp4dxDJsSQ1"
      },
      "source": [
        "## **One-Hot Encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4rN6lzztWFr"
      },
      "source": [
        "* One hot encoding method involves converting categorical information into a format that may be given to ML algorithms to help them perform better at prediction.\n",
        "\n",
        "* We employ one-hot encoding when there is no ordinal link between the variables. The model doesn't assume a natural ordering between categories when using One-Hot Encoding, which could lead to poor performance or unexpected results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skM71_eAtcZQ"
      },
      "outputs": [],
      "source": [
        "dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPAOpWcctuoV"
      },
      "outputs": [],
      "source": [
        "cols_to_encode = ['Gender', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage',\n",
        "                  'Age_Group','Policy_Sales_Channel_Categorical', 'Region_Code_Categorical']\n",
        "\n",
        "dataset = pd.get_dummies(data = dataset, columns=cols_to_encode)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh8VkzBpumxp"
      },
      "source": [
        "So, this concludes the dataset's Feature Selection part. Now we will use various machine learning algorithms to train the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKy4Tpypurbj"
      },
      "source": [
        "## **Machine Learning Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0gKTid6vBxx"
      },
      "source": [
        "Let's test out different machine learning models on our data set to evaluate how well they do. First, we will adjust the hyper-parameters of those models. Then, depending on elapsed time and evaluation metrics of the best parameters, we will compare and select the best model out of those.\n",
        "\n",
        "Types of Machine Learning Models we are going to train and evaluate our data set on:\n",
        "\n",
        "Decision Tree\n",
        "\n",
        "Gaussian Naive Bayes\n",
        "\n",
        "AdaBoost Classifier\n",
        "\n",
        "Bagging Classifier\n",
        "\n",
        "LightGBM\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "##**Hyperparameter Tunning methods:**\n",
        "\n",
        "We have tried different hyper-parameter tuning techniques. The models had to be trained for a very long time using GridSearchCV and RandomizedSearchCV, but all methods produced the same results. The model that required the least time to train and predict the results was halvingRandomizedSearchCV. Because of this, we strongly advise that you retain the Tuning_Method selected from the drop-down menu below as Halving_Randomized_Search_CV.\n",
        "\n",
        "To compare performance, we have also added a few model tweaking results from GridSearchCV and RandomizedSearchCV.\n",
        "\n",
        "## **Tunning Methods:-**\n",
        "\n",
        "* HalvingRandomizedSearchCV\n",
        "* GridSearchCV\n",
        "* RandomizedSearchCV\n",
        "\n",
        "## **Evaluation Metrices:-**\n",
        "\n",
        "* Accuracy Score\n",
        "* Precision\n",
        "* Recall\n",
        "* F1 Score\n",
        "* ROC AUC Score\n",
        "* Log Loss\n",
        "\n",
        "# **Plots:-**\n",
        "\n",
        "There is a one ROC Curve that displays the ROC Scores and a Parallel Coordinates Plot that displays all the hyper-parameter combinations used to tune the model to obtain the best parameters at the conclusion of hyper-parameter tuning for each model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVcJemLcyBjB"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix_and_roc_curves(model, X_test, y_test, y_pred):\n",
        "\n",
        "    fig, axes = plt.subplots(1,2, figsize=(22,5))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
        "                    cm.flatten()]\n",
        "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
        "                        cm.flatten()/np.sum(cm)]\n",
        "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
        "            zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    sns.heatmap(cm, ax = axes[0], annot=labels, fmt='',cmap='Blues')\n",
        "    axes[0].set_title('Confusion Matrix', fontdict={'fontsize': 16, 'fontweight':'bold'})\n",
        "\n",
        "    # predict probabilities\n",
        "    pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "    # roc curve for models\n",
        "    fpr, tpr, thresh = roc_curve(y_test, pred_proba[:,1], pos_label=1)\n",
        "\n",
        "    # roc curve for tpr = fpr\n",
        "    random_probs = [0 for i in range(len(y_test))]\n",
        "    p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # plot roc curves\n",
        "    plt.plot(fpr, tpr,linestyle='--',color='red', label = type(model).__name__)\n",
        "\n",
        "    plt.plot(p_fpr, p_tpr, linestyle='-', color='blue')\n",
        "    # title\n",
        "    plt.title('ROC curve', fontdict={'fontsize': 16, 'fontweight':'bold'})\n",
        "    # x label\n",
        "    plt.xlabel('False Positive Rate', fontdict={'fontsize': 12})\n",
        "    # y label\n",
        "    plt.ylabel('True Positive rate', fontdict={'fontsize': 12})\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualization(results_ds, parameters):\n",
        "\n",
        "    def shorten_param(param_name):\n",
        "        if \"__\" in param_name:\n",
        "            return param_name.rsplit(\"__\", 1)[1]\n",
        "        return param_name\n",
        "\n",
        "    column_results = [f\"param_{name}\" for name in parameters.keys()]\n",
        "    column_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
        "\n",
        "    results_ds = results_ds[column_results].sort_values(\"mean_test_score\", ascending=False)\n",
        "    results_ds = results_ds.rename(shorten_param, axis=1)\n",
        "\n",
        "    for col in results_ds.columns:\n",
        "        if col == 'param_random_state':\n",
        "            continue\n",
        "        try:\n",
        "            results_ds[col] = results_ds[col].astype(np.float64)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    fig = px.parallel_coordinates(\n",
        "    results_ds,\n",
        "    color=\"mean_test_score\",\n",
        "    color_continuous_scale=px.colors.sequential.Viridis,\n",
        "    title='Hyper Parameter Tuning',)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def evaluation_metrics(name, independent_feature_length , y_pred, y_test):\n",
        "\n",
        "    metrics_dict = {}\n",
        "    metrics_dict['Accuracy_Score'] = [accuracy_score(y_test,y_pred)]  #Accuracy Score\n",
        "    metrics_dict['Precision'] = [precision_score(y_test,y_pred)] #Precision\n",
        "    metrics_dict['Recall'] = [recall_score(y_test,y_pred)] #Recall\n",
        "    metrics_dict['F1_Score'] = [f1_score(y_test,y_pred)] #F1 Score\n",
        "    metrics_dict['ROC_AUC_Score'] = [roc_auc_score(y_test, y_pred)] #ROC AUC Score\n",
        "    metrics_dict['Log_Loss'] = [log_loss(y_test, y_pred)] #Log Loss\n",
        "\n",
        "    metrics_ds = pd.DataFrame(metrics_dict)\n",
        "\n",
        "    print(metrics_ds)\n",
        "\n",
        "\n",
        "def hyperparameter_tuning(x_train, y_train, model, parameters, tuning_model):\n",
        "\n",
        "    if tuning_model == 'Halving_Randomized_Search_CV':\n",
        "        tuned_model = HalvingRandomSearchCV(model, param_distributions = parameters, scoring = \"accuracy\", n_jobs=-1, factor=3, cv = 5 )\n",
        "\n",
        "    elif tuning_model == 'Randomized_Search_CV':\n",
        "        tuned_model = RandomizedSearchCV(model, param_distributions = parameters, scoring = 'accuracy', cv = 3, n_iter = 50, n_jobs=-1)\n",
        "\n",
        "    else:\n",
        "        tuned_mode = GridSearchCV(model, param_grid = parameters, scoring = 'accuracy', n_jobs=-1, cv = 3)\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    tuned_model.fit(x_train, y_train)\n",
        "\n",
        "    stop_time = time.time()\n",
        "\n",
        "    print('*****'*10+f'\\nBest Score for {type(model).__name__} : {tuned_model.best_score_}','\\n---')\n",
        "    print(f'Best Parameters for {type(model).__name__} : {tuned_model.best_params_}\\n'+'-----'*10)\n",
        "\n",
        "    print('Elapsed Time:',time.strftime(\"%H:%M:%S\", time.gmtime(stop_time - start_time)))\n",
        "    print('======'*5)\n",
        "\n",
        "    return tuned_model\n",
        "\n",
        "\n",
        "def perform_ml_algorithm(x_train, x_test, y_train, y_test, model, parameters, tuning_model):\n",
        "    print('-----'*10+f'\\n{type(model).__name__}\\n'+'-----'*10)\n",
        "\n",
        "    model.fit(x_train, y_train)\n",
        "    untuned_pred = model.predict(x_test)\n",
        "\n",
        "    # Evaluation Metrics before tuning\n",
        "    print(f'\\nEvaluation of {type(model).__name__} before tuning:\\n'+'-----'*10)\n",
        "    evaluation_metrics(type(model).__name__, len(list(x_train.columns)), untuned_pred, y_test)\n",
        "\n",
        "    print()\n",
        "    plot_confusion_matrix_and_roc_curves(model, x_test, y_test, untuned_pred)\n",
        "\n",
        "    # Hyper-parameter tuning\n",
        "    tuned_model = hyperparameter_tuning(x_train, y_train, model, parameters, tuning_model)\n",
        "    tuned_pred = tuned_model.predict(x_test)\n",
        "\n",
        "    # Evaluation Metrics after tuning\n",
        "    print(f'\\nEvaluation of {type(model).__name__} after tuning:\\n'+'-----'*10)\n",
        "    evaluation_metrics(type(model).__name__,len(list(x_train.columns)), tuned_pred, y_test)\n",
        "\n",
        "    print()\n",
        "    plot_confusion_matrix_and_roc_curves(tuned_model.best_estimator_, x_test, y_test, tuned_pred)\n",
        "    visualization(pd.DataFrame(tuned_model.cv_results_), parameters)\n",
        "\n",
        "\n",
        "def ml_algorithm_implementation(ds, model, parameters, tuning_model, feature_importance = False):\n",
        "\n",
        "    if feature_importance == False:\n",
        "        print('########'*8+'\\n     <<<< '+f'Tuning Model: {tuning_model}'+' >>>>\\n'+'********'*8)\n",
        "\n",
        "    x = dataset.iloc[:,1:]\n",
        "    y = dataset['Response']\n",
        "\n",
        "    # Train Test Split\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=57)\n",
        "\n",
        "    if feature_importance == True:\n",
        "        model.fit(x_train, y_train)\n",
        "        return x_train, y_train, model\n",
        "\n",
        "    perform_ml_algorithm(x_train, x_test, y_train, y_test, model, parameters, tuning_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRqFDCel1N78",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Keep it Halving_Randomized_Search_CV!! Other methods are time consuming.\n",
        "Tuning_Method = \"Halving_Randomized_Search_CV\" #@param [\"Halving_Randomized_Search_CV\", \"Grid_Search_CV\", \"Randomized_Search_CV\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuxM6YLK6Kjb"
      },
      "source": [
        "## **Decision Tree**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yLH78cw6UzH"
      },
      "source": [
        "Decision tree is a tool of sypervised learning algorithms used for solving classification and regression tasks.It builds a flowchart like tree structure where each internal node denotes a test on a attribute,each branch represents an outcome of a test, and each leaf node holds a class label.\n",
        "\n",
        "## **Hyper-Parameter Tuning:**\n",
        "\n",
        "splitter: The strategy used to choose the split at each node.\n",
        "\n",
        "max_depth: The maximum depth of the tree.\n",
        "\n",
        "min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
        "\n",
        "min_weight_fraction_leaf: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node.\n",
        "\n",
        "max_features: The number of features to consider when looking for the best split.\n",
        "\n",
        "max_leaf_nodes: Grow a tree with max_leaf_nodes in best-first fashion.\n",
        "\n",
        "random_state: Controls the randomness of the estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbUYCorLAs9K"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xd34_1v8r7p"
      },
      "outputs": [],
      "source": [
        "parameters_decision_tree = {\n",
        "    \"splitter\": [\"best\", \"random\"],\n",
        "    \"max_depth\": [None, 5, 7, 9],\n",
        "    \"min_samples_leaf\": [1, 2, 3, 4, 5],\n",
        "    \"min_weight_fraction_leaf\": [0.0, 0.3, 0.4, 0.5],\n",
        "    \"max_features\": [\"auto\", \"log2\", \"sqrt\", None],\n",
        "    \"max_leaf_nodes\": [None, 30, 40, 50, 60],\n",
        "    'random_state': [23]\n",
        "}\n",
        "\n",
        "ml_algorithm_implementation(dataset, DecisionTreeClassifier(), parameters_decision_tree, Tuning_Method, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6BLfG09dQ1h"
      },
      "source": [
        "## **Gaussian Naive Bayes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoOPHIDNgXvj"
      },
      "source": [
        "A variation of Naive Bayes that supports continuous data and follows to the Gaussian normal distribution is called Gaussian Naive Bayes. A class of supervised machine learning classification methods built on the Bayes theorem are known as naive bayes. Although it is a straightforward categorization method, it is highly functional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8iMTWuVge8L"
      },
      "source": [
        "## **Hyperparameter Tuning:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEkaSq6-g5iZ"
      },
      "source": [
        "var_smoothing: Portion of the largest variance of all features that is added to variances for calculation stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FODxap1ahCcZ"
      },
      "outputs": [],
      "source": [
        "parameters_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
        "\n",
        "ml_algorithm_implementation(dataset, GaussianNB(), parameters_NB, Tuning_Method, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlHOqH9GhTfe"
      },
      "source": [
        "## **AdaBoost Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl76LHc8hi7T"
      },
      "source": [
        "The Boosting technique known as AdaBoost algorithm, sometimes known as Adaptive Boosting, is used as an Ensemble Method in machine learning. The weights are redistributed to each instance, with higher weights being given to instances that were mistakenly identified, hence the name \"adaptive boosting.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m958ii42iJ7k"
      },
      "source": [
        "## **Hyper-Parameter Tuning:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PW2lKTpjdYF"
      },
      "source": [
        "**n_estimators**: The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n",
        "\n",
        "**learning_rate**: Weight applied to each classifier at each boosting iteration.\n",
        "\n",
        "**random_state**: Controls the randomness of the estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpnfjvXskFOe"
      },
      "outputs": [],
      "source": [
        "parameters_ada = {'n_estimators':[10, 100, 200,400],\n",
        "              'learning_rate':[0.001, 0.01, 0.1, 0.2, 0.5],\n",
        "              'random_state':[2]}\n",
        "\n",
        "ml_algorithm_implementation(dataset, AdaBoostClassifier(), parameters_ada, Tuning_Method, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRwDqri8khvC"
      },
      "source": [
        "## **Bagging Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LVmbLJvlN70"
      },
      "source": [
        "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AulAglELlgtm"
      },
      "source": [
        "### **Hyperparameter-Tunning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjUkAZmflxAB"
      },
      "source": [
        "**n_estimators**: The maximum number of estimators at which boosting is terminated.\n",
        "\n",
        "**random_state**: Controls the randomness of the estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_NRL02NmGbo"
      },
      "outputs": [],
      "source": [
        "parameters_bagging = {'n_estimators':[10, 100, 200, 400],\n",
        "                      'random_state':[26]}\n",
        "\n",
        "ml_algorithm_implementation(dataset, BaggingClassifier(), parameters_bagging, Tuning_Method, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wobtcqWh50h4"
      },
      "source": [
        "## **LightGBMClassifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISjJ0gYl7Enc"
      },
      "source": [
        "A distributed gradient boosting framework is known as LightGBM, or Light Gradient Boosting Machine.It is a fast algorithm since it makes use of Histogram-based Splitting, Gradient-based One-Side Sampling (GOSS), and Exclusive Feature Bundling (EFB)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FwO-5Jq7M_7"
      },
      "source": [
        "### **Hyperparameter Tuning:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_K6QAgW7ghW"
      },
      "source": [
        "**n_estimators**: Number of Boosting iterations.\n",
        "\n",
        "**learning_rate**: This setting is used for reducing the gradient step. It affects the overall time of training: the smaller the value, the more iterations are required for training.\n",
        "\n",
        "**min_data_in_leaf**: Minimal number of data in one leaf. Can be used to deal with over-fitting\n",
        "\n",
        "**random_state**: Controls the randomness of the estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbyHPoAM8Lfr"
      },
      "outputs": [],
      "source": [
        "parameters_lightgbm = {\n",
        "    'max_depths': np.linspace(1, 32, 32, endpoint=True),\n",
        "    'min_data_in_leaf':[100, 200, 250, 300],\n",
        "    'n_estimators':[50,100, 120,150,200],\n",
        "    'learning_rate':[.001,0.01,.1]\n",
        "}\n",
        "\n",
        "ml_algorithm_implementation(dataset, lgb.LGBMClassifier(), parameters_lightgbm, Tuning_Method, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdJivY7T8TVv"
      },
      "source": [
        "## **Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in_oa-Ch8cRt"
      },
      "source": [
        "The logistic classification model is a binary classification model in which the conditional probability of one of the two possible realizations of the output variable is assumed to be equal to a linear combination of the input variables, transformed by the logistic function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVn2IIMT8iVk"
      },
      "source": [
        "### **Hyperparameter Tunning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsSAKOLf8u22"
      },
      "source": [
        "**solver**: Algorithm to use in the optimization problem.\n",
        "\n",
        "**penalty**: Specify the norm of the penalty.\n",
        "\n",
        "**C**: Inverse of regularization strength\n",
        "\n",
        "**random_state**: Controls the randomness of the estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7zu2GtQ8__R"
      },
      "outputs": [],
      "source": [
        "parameters_logistic = {'solver' : ['newton-cg', 'lbfgs', 'liblinear','sag','saga'],\n",
        "                        'penalty' : ['l2'],\n",
        "                        'C' : [100, 10, 1.0, 0.1, 0.01, 0.001],\n",
        "                       'random_state':[2]}\n",
        "\n",
        "ml_algorithm_implementation(dataset, LogisticRegression(), parameters_logistic, Tuning_Method, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pg-7nHL9YU6"
      },
      "source": [
        "## **Best Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OztEcOG-cs3"
      },
      "source": [
        "We can infer that Bagging Classifier is the best model for our data set from all the above models that we tried to train and forecast the results. \"n_estimators\": 200 is the model's ideal parameter. Its scores are 0.85 for Accuracy, 0.31 for Precision, 0.15 for Recall, 0.20 for F1, 0.55 for ROC_AUC, and 4.98 for Log Loss. It has 03 minutes and 21 seconds of elapsed time.\n",
        "\n",
        "As can be seen, the Bagging Classifier does not have the highest Accuracy Score among our models. The issue with those models is that they have 0 Precision and Recall values, which implies that True Positives are also zero. This means that those algorithms cannot accurately forecast results if any buyer is prepared to purchase auto insurance.And as we all know,classification accuracy alone can be misleading if you had an unequal number of observations in each class.This is exactly the case with our data set.\n",
        "\n",
        "Hence,it is proved that Bagging classifier is the top model for our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyHjSitS_mtq"
      },
      "source": [
        "We might get a slight change in result everytime we run because we are using Halving_Randomized_Search_CV to perform hyperparameter tunning which randomely selects the combination of parameter to tune the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkBKYVaPAVxv"
      },
      "source": [
        "# **Extracting Feature Importance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "turyAZBbAzE8"
      },
      "source": [
        "With the values of its hyperparameters, we obtained our best model. Let's have a look at the importance of each feature in our data collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmUv2ckZA4S_"
      },
      "outputs": [],
      "source": [
        "def feature_plot(importances, X_train, y_train):\n",
        "\n",
        "    # Display the five most important features\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    columns = X_train.columns.values[indices[:5]]\n",
        "    values = importances[indices][:5]\n",
        "\n",
        "    # Creat the plot\n",
        "    fig = plt.figure(figsize = (9,5))\n",
        "    plt.title(\"Normalized Weights for First Five Most Predictive Features\", fontsize = 16)\n",
        "    plt.bar(np.arange(5), values, width = 0.2, align=\"center\", color = '#00A000', \\\n",
        "          label = \"Feature Weight\")\n",
        "    plt.bar(np.arange(5) - 0.2, np.cumsum(values), width = 0.2, align = \"center\", color = '#00A0A0', \\\n",
        "          label = \"Cumulative Feature Weight\")\n",
        "    plt.xticks(np.arange(5), columns)\n",
        "    plt.xlim((-0.5, 4.5))\n",
        "    plt.ylabel(\"Weight\", fontsize = 14)\n",
        "    plt.xlabel(\"Feature\", fontsize = 14)\n",
        "\n",
        "    plt.legend(loc = 'upper center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def show_feature_importance():\n",
        "    x_train, y_train, model = ml_algorithm_implementation(dataset, BaggingClassifier(n_estimators=200, random_state=23),\n",
        "                                None, None, True)\n",
        "\n",
        "    importances = np.mean([\n",
        "        tree.feature_importances_ for tree in model.estimators_\n",
        "        ], axis=0)\n",
        "    feature_plot(importances, x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GORVY7DhBXpi"
      },
      "outputs": [],
      "source": [
        "show_feature_importance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LooJKgJIBh4S"
      },
      "source": [
        "### **Observations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "891hpSmgB3Q-"
      },
      "source": [
        "* Annual_Premium_Treated has most impacted predictions\n",
        "* Gender_Male has highest feature weight but less cumulative weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VlsRjwQEZWe"
      },
      "source": [
        "## **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ii20M_FXV5"
      },
      "source": [
        "We started by looking for duplicates and null values after loading our dataset. Duplicates and null values weren't present, hence their treatment wasn't necessary. Before processing the data, we used feature scaling techniques to standardize the data and make it easier for ML algorithms to analyze.\n",
        "\n",
        "Through exploratory data analysis, we divided Age into three categories: young, middle-aged, and old, and we divided Region_Code into three categories: region_A, region_B, and region_C. The Policy Sales Channel was divided into channels A, B, and C. Further,we observed that youngAge customers are more interested in vehicle reaction. Customers with vehicles older than two years are more likely to be interested in auto insurance, according to our observation. Customers who have damaged cars are also more likely to be interested in auto insurance.\n",
        "\n",
        "For feature selection, we employed the Mutual Information approach for categorical features and Kendall's rank correlation coefficient for numerical features.Here, we observed that the dependent feature is mainly affected by the significant feature, Previously_Insured, and that there is no link between the two numerical features.\n",
        "\n",
        "Further, we applied Machine Learning Algorithms to determine whether a customer would be interested in Vehicle Insurance. For the Naive Bayes algorithm, we got an accuracy score of 68% and after hyperparameter tuning, the accuracy score increased to 72%. Similarly, for Decision Tree Classifier, AdaBoost, BaggingClassifier, LightGBM accuracy score was obtained around 82%-87%. So, we selected our best model as the model with an accuracy score of 85% considering precision and recall as we have an unequal number of observations in each class in our dataset, so accuracy alone can be deceiving."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}